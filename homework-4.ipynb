{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4. Конструирование текстовых признаков из твитов пользователей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый этап - сбор твитов пользователей. Необходимо подключаться к Twitter API и запрашивать твиты по id пользователя. \n",
    "Подключение к API подробно описано в ДЗ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pytagcloud import create_tag_image, make_tags, LAYOUT_MIX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"fPhtfgH5DPYJOR6kUeD45mN53\"\n",
    "CONSUMER_SECRET = \"gEbVdC1zu59Fd0f3SMeiqW2IhN55OEevW38aPe83ZncEOrG5fu\"\n",
    "\n",
    "ACCESS_TOKEN_KEY = \"269921529-YaLx73XrruQQNmHmHmQDpOcosjoYXISWr3cb1TA3\"\n",
    "ACCESS_TOKEN_SECRET = \"lMUMjkNH8lNT7YFaLfdtQRsUJtEPaL4fsxbAg2XYkaV0p\"\n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
    "                  consumer_secret=CONSUMER_SECRET, \n",
    "                  access_token_key=ACCESS_TOKEN_KEY, \n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения твитов пользователя может быть использован метод GetUserTimeline из библиотеки python-twitter. Он позволяет получить не более 200 твитов пользователя.\n",
    "\n",
    "Метод имеет ограничение по количеству запросов в секунду. Для получения информации о промежутке времени, которое необходимо подождать для повторного обращения к API может быть использован метод `GetSleepTime`. Для получения информации об ограничениях запросов с помощью метода `GetUserTimeLine` необходимо вызывать `GetSleepTime` с параметром \"statuses/user_timeline\".\n",
    "\n",
    "Метод GetUserTimeline возвращает объекты типа Status. У этих объектов есть метод AsDict, который позволяет представить твит в виде словаря.\n",
    "\n",
    "Id пользователей необходимо считать из файла, как было сделано в ДЗ 1.\n",
    "\n",
    "Необходимо реализовать функцию `get_user_tweets(user_id)`. Входной параметр - id пользователя из файла. Возвращаемое значение - массив твитов пользователя, где каждый твит представлен в виде словаря. Предполагается, что информация о пользователе содержится в твитах, которые пользователь написал сам. Это означает, что можно попробовать отфильтровать ответы другим пользователям, ссылки и ретвиты, а так же картинки и видео, так как наша цель - найти текстовую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>  984121344</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>  601849857</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>  351429761</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 2792643764</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>  215056389</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  class\n",
       "0   984121344      0\n",
       "1   601849857      0\n",
       "2   351429761      0\n",
       "3  2792643764      0\n",
       "4   215056389      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_SET_URL = \"https://kaggle2.blob.core.windows.net/competitions-data/inclass/4277/twitter_train.txt?sv=2012-02-12&se=2015-04-27T13%3A24%3A30Z&sr=b&sp=r&sig=dLCjlTEK1DiY7DOm0RrGB6eld47O2EuBLozuPeXAj80%3D\"\n",
    "df_users = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=0, names=[\"user_id\", \"class\"])\n",
    "df_users.head()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_user_tweets(user_id):\n",
    "    \"\"\"returns list of tweets as dicts\"\"\"\n",
    "    # your code here\n",
    "    try:\n",
    "        tweets = api.GetUserTimeline(user_id, count=200, trim_user=1, include_rts=0)\n",
    "        tweets_dict = [tweets[i].AsDict() for i in xrange(len(tweets))]\n",
    "        tweets = [tweets_dict[i][\"text\"] for i in xrange(len(tweets_dict))]\n",
    "    except twitter.TwitterError:\n",
    "        tweets = []\n",
    "    \n",
    "    return tweets\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets collected\n"
     ]
    }
   ],
   "source": [
    "#all_tweets = [None] * len(df_users[\"user_id\"])\n",
    "#f_name = \"tweets\"\n",
    "#f_ext = \".txt\"\n",
    "\n",
    "\"\"\"\n",
    "for i, user_id in enumerate(df_users[\"user_id\"][7500:]):\n",
    "    try:\n",
    "        sleep_time = api.GetSleepTime(\"statuses/user_timeline\")\n",
    "    except twitter.TwitterError:\n",
    "        sleep_time = 900\n",
    "       \n",
    "    if sleep_time > 0:\n",
    "        print \"sleep time = \", sleep_time, \" on j= \", j\n",
    "        time.sleep(sleep_time + 1)\n",
    "        print \"woke up\"\n",
    "    \n",
    "    j+=1\n",
    "    all_tweets[j] = get_user_tweets(user_id)\n",
    "    \n",
    "    if j % 200 == 0:\n",
    "        f_path = f_name + str(j) + f_ext\n",
    "        pickle.dump(all_tweets, open(f_path, \"wb\"))\n",
    "\n",
    "print \"finish. j= \", j \n",
    "\"\"\"    \n",
    "\n",
    "#2: j=4000 [:4001]\n",
    "\n",
    "print \"tweets collected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(all_tweets, open(\"tweets_full.txt\", \"wb\"))\n",
    "print \"written\"\n",
    "#tweets = pickle.loads(open(\"tweets_full.txt\", \"rb\").read())\n",
    "#print \"read\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбор текста твита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста предполагает разбиение текста на отдельные элементы - параграфы, предложения, слова. Мы будем преобразовывать текст твита к словам. Для этого текст необходимо разбить на слова. Сделать это можно, например, с помощью регулярного выражения.\n",
    "\n",
    "Необходимо реализовать функцию, `get_words(text)`. Входной параметр - строка с текстом. Возвращаемое значение - массив строк (слов). Обратите внимание, что нужно учесть возможное наличие пунктуации и выделить по возможности только слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    \"\"\"returns list of words\"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    return re.compile('\\w+').findall(text)\n",
    "    #return re.compile('[A-Za-z]+').findall(text)\n",
    "\n",
    "\n",
    "def get_words2(text):\n",
    "    \"\"\"returns list of words\"\"\"\n",
    "    # your code here\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'are', 'different', 'words']\n",
      "['Here', 'are', 'different', 'words']\n"
     ]
    }
   ],
   "source": [
    "print get_words(\"Here are different words!\")\n",
    "print get_words2(\"Here are different words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее полученные слова необходимо привести к нормальной форме. То есть привести их к форме единственного числа настоящего времени и пр. Сделать это можно с помощью библиотеки nltk. Информацию по загрузке, установке библиотеки и примерах использования можно найти на сайте http://www.nltk.org/\n",
    "\n",
    "Для загрузки всех необходимых словарей можно воспользоваться методом download из библиотеки nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://nltk.github.com/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшей обработки слова должны быть приведены к нижнему регистру. \n",
    "\n",
    "Для приведения к нормальной форме можно использовать `WordNetLemmatizer` из библиотеки nltk. У этого класса есть метод `lemmatize`.\n",
    "\n",
    "Также необходимо убрать из текста так называемые стоп-слова. Это часто используемые слова, не несущие смысловой нагрузки для наших задач. Сделать это можно с помощью `stopwords` из nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tokens(words)`. Входной параметр - массив слов. Возвращаемое значение - массив токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    \"\"\"returns list of tokens\"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    words = get_words(text)\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w.lower() for w in words if w.lower() not in stopwords]\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    #usage: lmtzr.lemmatize('cars')\n",
    "    words = [lmtzr.lemmatize(w) for w in content]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['different', u'word']\n"
     ]
    }
   ],
   "source": [
    "#print get_tokens([\"Here\", \"are\", \"Different\", \"words\"]) #without words = get_words(text) in get_tockens\n",
    "print get_tokens(\"Here are Different words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tweet_tokens(tweet)`. Входной параметр - текст твита. Возвращаемое значение -- токены твита. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweet_tokens(tweet):\n",
    "    # your code here\n",
    "    \n",
    "    #remove needless data - urls(https), replies(@), tags(#), numbers, words which consist of numbers and letters\n",
    "    \n",
    "    tweet = tweet.encode('ascii', 'ignore')\n",
    "    \n",
    "    tweet = re.sub(r'\\bhttps?://\\S*\\b', ' ', tweet) \n",
    "    tweet = re.sub(r'@\\b\\w+\\b', ' ', tweet)\n",
    "    tweet = re.sub(r'#\\b\\w+\\b', ' ', tweet)\n",
    "    tweet = re.sub(r'\\b\\d+\\b', ' ', tweet)\n",
    "    tweet = re.sub(r'\\b\\w*\\d+\\w*\\b', ' ', tweet)\n",
    "        \n",
    "    return get_tokens(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['could', 'use', 'help', 'french', 'korean', 'russian', u'translation', 'please']\n",
      "['set', 'ready', 'go']\n",
      "['go', 'go', 'level']\n",
      "['monday', 'year']\n",
      "['waiting']\n"
     ]
    }
   ],
   "source": [
    "print get_tweet_tokens(\"We could use some help with French, Korean & Russian translations please. http://tweetsofwarcraft.com/help/ #WorldofWarcraft #WoW\")\n",
    "print get_tweet_tokens(\"All set up @AshtonKCJ #AKCJconf14 ready to go! http://t.co/WYpiFwTumu\")\n",
    "print get_tweet_tokens(\"@kalla_lily Go go, 1 more level to do :)\")\n",
    "print get_tweet_tokens(\"is on a Monday this year... \\U0001f62d #whyyyyy\")\n",
    "print get_tweet_tokens(\"56min waiting 4you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `collect_users_tokens()`. Функция должна сконструировать матрицу признаков пользователей. В этой матрице строка - пользователь. Столбец - токен. На пересечении - сколько раз токен встречается у пользователя.\n",
    "Для построения матрицы можно использовать `DictVectorizer` из `sklearn.feature_extraction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_users_tokens(df_users, tweets):\n",
    "    \"\"\"returns users list and list of user dicts. Each dict contains frequence of user tokens\"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    users_tokens_dictionary = []\n",
    "    \n",
    "    for user_tweets in tweets:\n",
    "        tweets_tokens = [get_tweet_tokens(user_tweet) for user_tweet in user_tweets]\n",
    "        user_tokens = [token for tweet_tokens in tweets_tokens for token in tweet_tokens]\n",
    "        \n",
    "        user_dict = dict.fromkeys(user_tokens) #eliminates repeated tokens\n",
    "\n",
    "        for key in user_dict:\n",
    "            user_dict[key] = user_tokens.count(key)\n",
    "    \n",
    "        users_tokens_dictionary.append(user_dict)\n",
    "    \n",
    "    return df_users[\"user_id\"], users_tokens_dictionary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users tokens collected ...\n",
      "... and transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2507: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#users, users_tokens = collect_users_tokens(df_users, tweets)\n",
    "print \"users tokens collected ...\"\n",
    "v = DictVectorizer()\n",
    "vs = v.fit_transform(users_tokens)\n",
    "print \"... and transformed\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Сохраним полученные данные в файл. Используется метод savez из numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to out_4.dat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.savez(\"files/out_4.dat\", data=vs, users=users, users_tokens=users_tokens )\n",
    "print \"saved to out_4.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для получения представления о полученной информацию о токенах предлагается отобразить ее в виде облака тэгов. [Подсказка](http://anokhin.github.io/img/tag_cloud.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count number of tags and their occurrence in all tweets\n",
    "\n",
    "features = v.get_feature_names()\n",
    "\n",
    "vs_count = np.array(vs.sum(axis=0)).flatten()\n",
    "\n",
    "tag_count = [None] * len(features)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    tag_count[i] = [feature, vs_count[i]]\n",
    "\n",
    "pickle.dump(tag_count, open(\"tags.txt\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_tag_cloud(tag_count):\n",
    "    \"\"\"Draws tag cloud of found tokens\"\"\"\n",
    "    # your code here\n",
    "    values = np.empty(len(tag_count))\n",
    "\n",
    "    for i in xrange(len(tag_count)):\n",
    "        values[i] = tag_count[i][1]\n",
    "\n",
    "    n = 100 #100 tags in cloud\n",
    "    n_frequent_values = np.argsort(-values)[:n]\n",
    "    n_frequent_tags = [[tag_count[i][0], tag_count[i][1]] for i in n_frequent_values]\n",
    "\n",
    "    tags = make_tags(n_frequent_tags, 100)\n",
    "    create_tag_image(tags, 'tag_cloud2.png', size=(1300, 900),  fontname='Cardo')\n",
    "\n",
    "    return\n",
    "    \n",
    "#Fontname should be one of Nobile, Old Standard TT, Cantarell, Reenie Beanie, Cuprum, Molengo, \n",
    "#Neucha, Philosopher, Yanone Kaffeesatz, Cardo, Neuton, Inconsolata, Crimson Text, \n",
    "#Josefin Sans, Droid Sans, Lobster, IM Fell DW Pica, Vollkorn, Tangerine, Coustard, PT Sans Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_tag_cloud(tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "print \"Finish\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
